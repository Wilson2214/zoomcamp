Data Engineering ZoomCamp WeekÂ 4

Week 4 of the Data Engineering ZoomCamp was focused on analytics engineering with dbt. Analytics engineering as a role generally sits between data engineering and data analysis. The goal of analytics engineering is to introduce good software engineering practices to the efforts of the data analysts and data scientists. To learn more about the Data Engineering ZoomCamp, sign up for the next cohort, or walk through the videos at your own pace, check out the Data Talks website. My code and notes will be available on GitHub.

ETL vs ELT

ETL (extract, transform, load) is the process of extracting from a source system, performing transformations, then loading into a data warehouse for later usage. This is generally more stable and compliant for data analysis, but leads to higher storage and compute costs. ELT (extract, load, transform) instead directly loads data into a data warehouse, meaning transformations are done once in the data warehouse. This is faster and allows for more flexibility as well as generally lower costs.

I think there is an interesting discussion to be had on the merits of ETL vs ELT, but much of that discussion is based on historical ideas of storing data. Cloud computing has drastically decreased the cost of storing data to the point of allowing companies to store nearly unlimited amount of raw data. With this in mind, ELT makes sense as we can easily store large quantities of raw data in our warehouse to later transform. In practice though, we are likely running multiple loads and transforms throughout a pipeline. In the case of this course we technically run an ETL process to extract raw data from an API in parquet format. We can then directly store these parquet files on Google Cloud object storage or we can save it directly to the Big Query Data Warehouse in its raw form to be later transformed. Even though our transformations will occur after being loaded to GCP or Big Query, the former would be considered ETL and the latter ELT. The true difference seems to be that ELT uses SQL to transform data in a warehouse and ETL can use other systems. 

As touched on in previous weeks, a major component of the ETL/ELT process is data compatibility and schema. In the homework for this week, I ran into a number of schema issues related to the raw parquet data. In particular, certain attributes were either integer or numeric in different parquet files. When attempting to merge in the data warehouse, compatabilty issues resulted in errors. In a case like this, an ETL process could be beneficial because we could enforce schema or otherwise alter the ingested files before loading into our system. ETL is often beneficial for structured data like this because of schema issues. ELT on the other hand would be beneficial if we were ingesting unstructured data like images or documents. We likely would not run into schema issues when later interacting with this data because it is unstructured and would need to be altered to fit our needs later in an analysis process.

Overall, each project you are working on should compare the costs and benefits of using ETL or ELT. For example, if you are ingesting large amounts of data from an API, it may be easier to simply ingest this raw data, then utilize and ELT tool like dbt to perform transformations, taking advantage of massive parallel engines and SQL efficiencies. On the other hand, if your data requires transformations that are not easily computed with SQL or you need to decrease the size or protect PII by dropping data, it may be easier to perform the transformation in-line.

Data Modeling Concepts

The goal with data modeling is to deliver understandable data to the end user while also delivering fast query performance. The most common method of data modeling is a star schema. Star schema consists of fact and dimension tables. Fact tables are actual metrics or measurements which correspond to a business process (e.g. an individual sale). Dimension tables correspond to a business entity and provide context to a business process (e.g. customer or product associated with a sale).

Often times, data models are either never built or are built as the product or project moves along. What should happen is starting a project with a conceptual data model. This conceptual model should explain what data is available, how it relates to other data, and what requirements the project that interacts with this data will need. This model will later inform a physical data model that includes actual tables and attributes and that will take into account the technical environment and performance considerations. Once this is all laid out, we can easily begin our ETL/ELT process. A conceptual model really represents the organizational structure of data and can be thought of as a more general view of the data whereas the physical model is the true layout of a database or warehouse.

dbt

dbt is a transformation tool that resides within a data warehouse that allows us to easily transform data with SQL. It allows users to deploy analytics code following software engineering best practices by defining a development workflow. A dbt model is a sql file with a specific select statement. The result of this file is then stored back in the data warehouse as a new table that can be used downstream. dbt is beneficial because we can add SQL macros and adapters to further transform data with SQL that would be difficult to create on our own. It also provides the ability to add yml files for schema declarations and incorporate testing to confirm that transformations are working properly and data is formatted correctly.

The main benefit of dbt is that data transformation can be performed more efficiently by processing at the database level through SQL commands. Data does not have to be read into a different system (e.g. loaded into python via pandas for tranformations) making the entire transformation process faster, more secure, and easier to maintain. Additionally, dbt enforces good coding practices by incorporating testing, modular code, and continuous integration. It also maintains data documentation and provides visual representations of pipelines.

My initial reaction to dbt is questioning why I would actually use it in my day-to-day work. Currently I transform most of my data with PySpark so that it is available for vizualizations later in the pipeline. From my research, the biggest reason dbt is used is to target a different audience than a normal data engineering audience. dbt has a much lower barrier to entry than something like building ETL pipelines with Python. Most users of dbt can easily build pipelines with unit testing and documentation without needing advanced programming knowledge. Analyts can easily create versioned code quickly which can impact a business greatly. For a team that writes a lot of SQL code to generate OLAP tables and visualizations, dbt is hugely beneficial. It incorporates versioning and out of the box testing for these users. 